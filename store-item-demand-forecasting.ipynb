{"cells":[{"metadata":{"_uuid":"d8315afd07bc0251a225deb67cca33c72fec349c"},"cell_type":"markdown","source":"# Forcasting Demand-LSTM"},{"metadata":{"_uuid":"bc5a117d1906dfc16c479829e08b2be250965b2b"},"cell_type":"markdown","source":"* Importing required labraries"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport bokeh\nfrom bokeh.io import show\nfrom bokeh.models import CustomJS, ColumnDataSource, Slider, Label, Div, HoverTool, Band, Span, BoxAnnotation\nfrom bokeh.plotting import figure\nfrom bokeh.palettes import Spectral11\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nfrom IPython.display import display\nimport re\nimport statsmodels\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\nimport statsmodels.api as sm\nfrom pandas.plotting import autocorrelation_plot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import Flatten\nimport warnings\nwarnings.filterwarnings('ignore', module='matplotlib')\nbokeh.io.output_notebook()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2a63faa7b3f65d8a3524931e06485bb29e509c1"},"cell_type":"markdown","source":"* Loading Data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"df_train=pd.read_csv('../input/train.csv',parse_dates=['date'])\ndf_train.sales = df_train.sales.astype(np.float)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7e6af983648ee3683fba644736a7726876272274"},"cell_type":"markdown","source":"# Exploratory Data Analysis"},{"metadata":{"trusted":true,"_uuid":"f94fc6ac42194e61beb0a7eae94fdccaf6259a0e"},"cell_type":"markdown","source":"getting idea abot the dataset"},{"metadata":{"trusted":true,"_uuid":"5058dece8ffb7d12279a400b284b3a722911da78"},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bf3ac57a18635fc665508df3a9fa4048513c7607"},"cell_type":"markdown","source":"* Check for missing data"},{"metadata":{"trusted":true,"_uuid":"e166267c728cf72dfb7fd8e36e582a779c5fa2ac"},"cell_type":"code","source":"print(df_train.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5491a29cd3587d89da2875a85509636d23877b2a"},"cell_type":"markdown","source":"* **Plot Data**\n"},{"metadata":{"_uuid":"0a2568674afeb5275cd6c7a632f6aea90af9994e"},"cell_type":"markdown","source":"Let's plot sales time series to get some general ideas about the data. The data is averaged (weekly) to make plots more readable"},{"metadata":{"trusted":true,"_uuid":"358327c59562fee6db361608895f6619bf730e7d"},"cell_type":"code","source":"i1, i2 = '1_1', '2_1'\ndf_train['it_sa'] = df_train.item.astype(str) + '_' + df_train.store.astype(str) \nsales = df_train.pivot(index='date', columns='it_sa', values='sales').resample('W').mean()\ndf_train.drop(columns=['it_sa'], inplace=True)\n# display(sales_w.head(3))\nsales_source = sales.loc[:, [i1, i2]].copy()\nsource = ColumnDataSource(data=sales_source)\nsource_ref = ColumnDataSource(data=sales)\np1 = figure(plot_width=750, plot_height=150, title=i1, x_axis_type='datetime', tools=\"pan,wheel_zoom,reset\")\nline1 = p1.line(x='date', y=i1, source=source)\np2 = figure(plot_width=750, plot_height=150, title=i2, x_axis_type='datetime', tools=p1.tools,\n            x_range=p1.x_range)\nline2 = p2.line('date', i2, source=source)\np1.add_tools(HoverTool(tooltips=[('sales', '@{}'.format(i1)), ('vs.', '@{}'.format(i2))], \n                       renderers=[line1, line2], mode='vline'))\np2.add_tools(HoverTool(tooltips=[('sales', '@{}'.format(i2)), ('vs.', '@{}'.format(i1))], \n                       renderers=[line1, line2], mode='vline'))\n\nslider_it1 = Slider(start=1, end=50, value=1, step=1, title=\"item a\", callback_policy=\"mouseup\")\nslider_it2 = Slider(start=1, end=50, value=1, step=1, title=\"item b\")\nslider_sa1 = Slider(start=1, end=10, value=1, step=1, title=\"store a\")\nslider_sa2 = Slider(start=1, end=10, value=1, step=1, title=\"store b\")\njs_code = \"\"\"\n    var v = cb_obj.value;\n    var y_old = source.data['{old}'];\n    var y_new = ref.data[{new}];\n    for (var i = 0; i < y_old.length; i++) {\n        y_old[i] = y_new[i];\n    }\n    source.change.emit();\n\"\"\"\ncallback_it1 = CustomJS(args=dict(source=source, ref=source_ref, s=slider_sa1), code=js_code.replace('{old}', i1).replace('{new}', 'v + \"_\" + s.value'))\ncallback_it2 = CustomJS(args=dict(source=source, ref=source_ref, s=slider_sa2), code=js_code.replace('{old}', i2).replace('{new}', 'v + \"_\" + s.value'))\ncallback_sa1 = CustomJS(args=dict(source=source, ref=source_ref, s=slider_it1), code=js_code.replace('{old}', i1).replace('{new}', 's.value + \"_\" + v'))\ncallback_sa2 = CustomJS(args=dict(source=source, ref=source_ref, s=slider_it2), code=js_code.replace('{old}', i2).replace('{new}', 's.value + \"_\" + v'))\nslider_it1.js_on_change('value', callback_it1)\nslider_it2.js_on_change('value', callback_it2)\nslider_sa1.js_on_change('value', callback_sa1)\nslider_sa2.js_on_change('value', callback_sa2)\n\nlayout = bokeh.layouts.column(Div(text='<h4>Sales Volumes TS (Bokeh)</h4>'),\n                              bokeh.layouts.row(slider_it1, slider_sa1), p1, \n                              bokeh.layouts.row(slider_it2, slider_sa2), p2)\nbokeh.io.show(layout)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f37dd094e38be21fb9b34a9fa3450b1ff46aa01"},"cell_type":"markdown","source":"* There is a clear Seasionality and trend in all the items and stores "},{"metadata":{"trusted":true,"_uuid":"fd74cc3ccaa3e8156d1f9567b1b4ac91ae62e6be"},"cell_type":"markdown","source":"# Trend and Seasionality"},{"metadata":{"_uuid":"3b6267abd3ccde0b78080296b47d7d3341d82c76"},"cell_type":"markdown","source":"* Adding Year,Month,Day,Day of week, Day of year and week of year as feature to the dataset"},{"metadata":{"trusted":true,"_uuid":"f252f60a1719839060eb60d5d826f1d967df4257"},"cell_type":"code","source":"def add_datepart(df, fldname, inplace=False, drop=False):\n    if not inplace: df = df.copy()        \n    fld = df[fldname]\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n    targ_pre = re.sub('[Dd]ate$', '', fldname)\n    \n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear','Weekofyear']\n\n    for n in attr: \n        df[targ_pre + n] = getattr(fld.dt, n.lower())\n    if drop: \n        df.drop(fldname, axis=1, inplace=True)\n    if not inplace: return df \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1bc4aca28181656ac1c1c745bea5ef777b10b58"},"cell_type":"code","source":"df_trainext = add_datepart(df_train, 'date', inplace=False)\ndisplay(df_trainext.head(3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fc8710d52a11ef5b943af9360258c56ef77ec880"},"cell_type":"code","source":"df_trainext.groupby('date').mean()['sales'].plot(figsize=(12,3), title='Sales (aggregated data)')\n\nfig, ax = plt.subplots(2, 2, figsize=(10, 10))\n_ = pd.pivot_table(df_trainext, values='sales', columns='Year', index='Month').plot(title=\"Yearly seasonality\", ax=ax[0,0])\n_ = pd.pivot_table(df_trainext, values='sales', columns='Month', index='Day').plot(title=\"Monthly seasonality\", ax=ax[0,1])\n_ = pd.pivot_table(df_trainext, values='sales', columns='Year', index='Dayofweek').plot(title=\"Weekly seasonality (by year)\", ax=ax[1,0])\n_ = pd.pivot_table(df_trainext, values='sales', columns='Month', index='Dayofweek').plot(title=\"Weekly seasonality (by month)\", ax=ax[1,1])\nfig.suptitle('Patern of Seasionality of Sales (aggregated data)')\nfig.tight_layout(rect=[0, 0, 1, 0.96])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d7cb7458daff41d0d95c469f62c0a66644c2e133"},"cell_type":"code","source":"_ = pd.pivot_table(df_trainext, values='sales', index='Year').plot(style='-o', title=\"Annual trend (aggregated data)\")\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66c3581fc8a6a72fc2e253c9d7a708624944f213"},"cell_type":"markdown","source":" * **Seasonal Trend Decomposition** using **Loess (STL)** decomposition"},{"metadata":{"trusted":true,"_uuid":"2d1ba397012a1a28d0eea6f34175e2a84a52096e"},"cell_type":"code","source":"freq_season_mapping = {'None':None, 'Weekly': 7, 'Monthly':30, 'Yearly':365}\n\ndef update_stl_decompose(i_num, s_num, seasonality='Yearly', stl_style='additive'):\n    ts = df_train.query('store == @s_num & item == @i_num').set_index('date')['sales']\n    freq = freq_season_mapping[seasonality]\n    \n    fig, ax = plt.subplots(4, 1, figsize=(12,10))\n    decomposition = sm.tsa.seasonal_decompose(ts, model=stl_style, freq=freq)\n    _ = decomposition.observed.plot(ax=ax[0], title='observed')\n    _ = decomposition.trend.plot(ax=ax[1], title='trend')\n    _ = decomposition.seasonal.plot(ax=ax[2], title='seasonal')\n    _ = decomposition.resid.plot(ax=ax[3], title='residual')\n    res = decomposition.resid.values\n    res = res[np.isfinite(res)]\n# #     adfuller_stat = statsmodels.tsa.stattools.adfuller(res)\n#     ljungbox_stat = statsmodels.stats.diagnostic.acorr_ljungbox(res)\n#     statsmodels.graphics.tsaplots.plot_pacf(res, ax=ax[4], lags=40, \n#                                            title='residuals pacf; ljung-box p-value = {:.2E} / {:.2E}'.format(ljungbox_stat[1][6], \n#                                                                                                               ljungbox_stat[1][30]))\n    fig.suptitle('STL decomposition')\n    fig.tight_layout(rect=[0, 0, 1, 0.96])\n    \ns_slider = widgets.IntSlider(min=1, max=10, continuous_update=False, description='store', layout={'width': '2.1in', 'height': '1in'})\ni_slider = widgets.IntSlider(min=1, max=50, continuous_update=False, description='item', layout={'width': '2.1in', 'height': '1in'})\nseason_drop = widgets.Dropdown(value='Yearly', options=['Weekly', 'Monthly', 'Yearly'], description='seasonality', layout={'width': '2.1in'})\nstltype_drop = widgets.Dropdown(value='multiplicative', options=['additive', 'multiplicative'], description='STL type', layout={'width': '2.1in'})\nui = widgets.HBox([s_slider, i_slider, season_drop, stltype_drop], layout={'min_width': '6in', 'max_width': '6in'})\nout = widgets.interactive_output(update_stl_decompose, {'s_num': s_slider, 'i_num': i_slider, \n                                                        'seasonality': season_drop, 'stl_style': stltype_drop})\ndisplay(ui, out)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3794bc73e35441f850b4b0dbf4cbe3c0eb2bc59"},"cell_type":"markdown","source":"* Lag Auto-Correlation plot"},{"metadata":{"trusted":true,"_uuid":"698f61ef97d69ab22341f06d951edcf27cefbcdf"},"cell_type":"code","source":"autocorrelation_plot(pd.pivot_table(df_trainext,values='sales',index='Day'))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d1395d853453eb5addaf3509da9d826fec95a2c"},"cell_type":"markdown","source":"# Training Forecast Model by Encoder-Decoder LSTM"},{"metadata":{"trusted":true,"_uuid":"40c8f00d5c4f646ee50d1910743b08333731fc0a"},"cell_type":"code","source":"def split_dataset(data,hor=7):\n    # split into standard weeks\n    data=data.iloc[6:]\n    data_split=np.array(np.split(data.values, round(len(data)/hor)))\n    train,test=data_split[:248],data_split[248:]\n#     train, test = data.loc[:'2017-10-01'], data.loc['2017-10/-01':]\n    # restructure into windows of weekly data\n#     train = np.array(np.split(train.values, round(len(train)/hor)))\n#     test = np.array(np.split(test.values, round(len(test)/hor)))\n    return train, test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"311f531f98f08f67c7a62aa95cc94473c1008bfd"},"cell_type":"code","source":"data_agg=pd.pivot_table(df_train,values='sales',index='date')\ntrain,test=split_dataset(data_agg,7)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8a47ce7801d37595a95c16935ecf107de5af3599"},"cell_type":"code","source":"# evaluate one or more weekly forecasts against expected values\ndef evaluate_forecasts(actual, predicted):\n    scores = list()\n    # calculate an RMSE score for each day\n    for i in range(actual.shape[1]):\n        # calculate mse\n        mse = mean_squared_error(actual[:, i], predicted[:, i])\n        # calculate rmse\n        rmse = sqrt(mse)\n        # store\n        scores.append(rmse)\n    # calculate overall RMSE\n    s = 0\n    for row in range(actual.shape[0]):\n        for col in range(actual.shape[1]):\n            s += (actual[row, col] - predicted[row, col])**2\n    score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n    return score, scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1dbd2d3175711535ee4b091ce4204567a17bc07"},"cell_type":"code","source":"# evaluate a single model\ndef evaluate_model(train, test, n_input):\n    # fit model\n    model = build_model(train, n_input)\n    # history is a list of weekly data\n    history = [x for x in train]\n    # walk-forward validation over each week\n    predictions = list()\n    for i in range(len(test)):\n        # predict the week\n        yhat_sequence = forecast(model, history, n_input)\n        # store the predictions\n        predictions.append(yhat_sequence)\n        # get real observation and add to history for predicting the next week\n        history.append(test[i, :])\n    # evaluate predictions days for each week\n    predictions = np.array(predictions)\n    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n    return score, scores,model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a108cf21f8b82b979375a7f85ae7d88078d7fdd"},"cell_type":"code","source":"# summarize scores\ndef summarize_scores(name, score, scores):\n    s_scores = ', '.join(['%.1f' % s for s in scores])\n    print('%s: [%.3f] %s' % (name, score, s_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc1ad987b34532d672144337b14bf216a5b187a9"},"cell_type":"code","source":"# convert history into inputs and outputs\ndef to_supervised(train, n_input, n_out=7):\n    # flatten data\n    data = train.reshape((train.shape[0]*train.shape[1], train.shape[2]))\n    X, y = list(), list()\n    in_start = 0\n    # step over the entire history one time step at a time\n    for _ in range(len(data)):\n        # define the end of the input sequence\n        in_end = in_start + n_input\n        out_end = in_end + n_out\n        # ensure we have enough data for this instance\n        if out_end < len(data):\n            x_input = data[in_start:in_end, 0]\n            x_input = x_input.reshape((len(x_input), 1))\n            X.append(x_input)\n            y.append(data[in_end:out_end, 0])\n        # move along one time step\n        in_start += 1\n    return np.array(X), np.array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"78eb2c893a8520c633f47d1ad296b479493ef560"},"cell_type":"code","source":"# make a forecast\ndef forecast(model, history, n_input):\n    # flatten data\n    data = np.array(history)\n    data = data.reshape((data.shape[0]*data.shape[1], data.shape[2]))\n    # retrieve last observations for input data\n    input_x = data[-n_input:, 0]\n    # reshape into [1, n_input, 1]\n    input_x = input_x.reshape((1, len(input_x), 1))\n    # forecast the next week\n    yhat = model.predict(input_x, verbose=0)\n    # we only want the vector forecast\n    yhat = yhat[0]\n    return yhat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"976f28a55dab8e1d66303a390e09ad842d40b949"},"cell_type":"code","source":"def build_model(train, n_input):\n    # prepare data\n    train_x, train_y = to_supervised(train, n_input)\n    # define parameters\n    verbose, epochs, batch_size = 0, 25, 16\n    n_timesteps, n_features, n_outputs = train_x.shape[1], train_x.shape[2], train_y.shape[1]\n    # reshape output into [samples, timesteps, features]\n    train_y = train_y.reshape((train_y.shape[0], train_y.shape[1], 1))\n    # define model\n    model = Sequential()\n    model.add(LSTM(300, activation='relu', input_shape=(n_timesteps, n_features)))\n    model.add(RepeatVector(n_outputs))\n    model.add(LSTM(300, activation='relu', return_sequences=True))\n    model.add(TimeDistributed(Dense(100, activation='relu')))\n    model.add(TimeDistributed(Dense(1)))\n    model.compile(loss='mse', optimizer='adam')\n    # fit network\n    model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, verbose=verbose)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c1ba098aa4af69a078a339d897af36cfe9a970c1"},"cell_type":"code","source":"# evaluate model and get scores\nn_input = 7\nscore, scores,model = evaluate_model(train, test, n_input)\n# summarize scores\nsummarize_scores('lstm', score, scores)\n# plot scores\ndays = [ 'Sunday', 'Monday', 'Tuesday', 'Wednsday', 'Thursday', 'Friday','Saturday']\nplt.plot(days, scores, marker='o', label='lstm')\nplt.ylabel('RMSE')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d89d551c819d7b359463122673091bb10c0cbcf0"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}